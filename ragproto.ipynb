{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUzhjpVfrZdf",
        "outputId": "dd3fbb2c-7aa7-4702-8eb0-a0f53586bd20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.51-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2024.2.2)\n",
            "Installing collected packages: importlib_metadata, fastavro, backoff, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed backoff-2.2.1 cohere-4.51 fastavro-1.9.4 importlib_metadata-6.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hnswlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl4y5Oi_8kZ8",
        "outputId": "35063268-dfa8-4726-a14c-9c78065cef08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hnswlib\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hnswlib) (1.25.2)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287616 sha256=023067b5591210e41108fe92ada9032295c91f6758279a3fff32353bac09ef15\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: hnswlib\n",
            "Successfully installed hnswlib-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99V1DuUB9BH0",
        "outputId": "bdb40ea2-99b9-4558-fab0-9e00c94895b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.12.5-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.10.0)\n",
            "Collecting unstructured-client>=0.15.1 (from unstructured)\n",
            "  Downloading unstructured_client-0.21.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.6)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow>=3.19.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=1.0.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.16.0)\n",
            "Collecting typing-inspect>=0.9.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.2)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=d10963c15c31199ae83eae81d3ebee1cb3a15a3f695605f6d89d4ef35d8129a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, mypy-extensions, marshmallow, langdetect, jsonpath-python, emoji, typing-inspect, dataclasses-json, unstructured-client, unstructured\n",
            "Successfully installed dataclasses-json-0.6.4 emoji-2.10.1 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.21.0 mypy-extensions-1.0.0 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.6.1 typing-inspect-0.9.0 unstructured-0.12.5 unstructured-client-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B4I2iV--rHid"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import os\n",
        "import hnswlib\n",
        "import json\n",
        "import json\n",
        "import uuid\n",
        "from typing import List, Dict\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "api_key = \"g8rkyTzW6QwzXaRDBQCVKIClLG8RfmP06GQZibdn\"\n",
        "co = cohere.Client(api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Documents:\n",
        "\n",
        "    def __init__(self, sources: List[Dict[str, str]]):\n",
        "        self.sources = sources\n",
        "        self.docs = []\n",
        "        self.docs_embs = []\n",
        "        self.retrieve_top_k = 10\n",
        "        self.rerank_top_k = 3\n",
        "        self.load()\n",
        "        self.embed()\n",
        "        self.index()\n",
        "\n",
        "\n",
        "    def load(self) -> None:\n",
        "        \"\"\"\n",
        "        Loads the documents from the sources and chunks the HTML content.\n",
        "        \"\"\"\n",
        "        print(\"Loading documents...\")\n",
        "\n",
        "        for source in self.sources:\n",
        "            elements = partition_html(url=source[\"url\"])\n",
        "            chunks = chunk_by_title(elements)\n",
        "            for chunk in chunks:\n",
        "                self.docs.append(\n",
        "                    {\n",
        "                        \"title\": source[\"title\"],\n",
        "                        \"text\": str(chunk),\n",
        "                        \"url\": source[\"url\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    def embed(self) -> None:\n",
        "        \"\"\"\n",
        "        Embeds the documents using the Cohere API.\n",
        "        \"\"\"\n",
        "        print(\"Embedding documents...\")\n",
        "\n",
        "        batch_size = 90\n",
        "        self.docs_len = len(self.docs)\n",
        "\n",
        "        for i in range(0, self.docs_len, batch_size):\n",
        "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
        "            texts = [item[\"text\"] for item in batch]\n",
        "            docs_embs_batch = co.embed(\n",
        "\t\t              texts=texts,\n",
        "                      model=\"embed-english-v3.0\",\n",
        "                      input_type=\"search_document\"\n",
        "\t \t\t).embeddings\n",
        "            self.docs_embs.extend(docs_embs_batch)\n",
        "\n",
        "    def index(self) -> None:\n",
        "        \"\"\"\n",
        "        Indexes the documents for efficient retrieval.\n",
        "        \"\"\"\n",
        "        print(\"Indexing documents...\")\n",
        "\n",
        "        self.index = hnswlib.Index(space=\"ip\", dim=1024)\n",
        "        self.index.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
        "        self.index.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
        "\n",
        "        print(f\"Indexing complete with {self.index.get_current_count()} documents.\")\n",
        "\n",
        "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Retrieves documents based on the given query.\n",
        "\n",
        "        Parameters:\n",
        "        query (str): The query to retrieve documents for.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict[str, str]]: A list of dictionaries representing the retrieved  documents, with 'title', 'snippet', and 'url' keys.\n",
        "        \"\"\"\n",
        "        docs_retrieved = []\n",
        "        query_emb = co.embed(\n",
        "                    texts=[query],\n",
        "                    model=\"embed-english-v3.0\",\n",
        "                    input_type=\"search_query\"\n",
        "                    ).embeddings\n",
        "\n",
        "        doc_ids = self.index.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
        "        docs_to_rerank = []\n",
        "        for doc_id in doc_ids:\n",
        "            docs_to_rerank.append(self.docs[doc_id][\"text\"])\n",
        "\n",
        "        rerank_results = co.rerank(\n",
        "            query=query,\n",
        "            documents=docs_to_rerank,\n",
        "            top_n=self.rerank_top_k,\n",
        "            model=\"rerank-english-v2.0\",\n",
        "        )\n",
        "\n",
        "        doc_ids_reranked = []\n",
        "        for result in rerank_results:\n",
        "            doc_ids_reranked.append(doc_ids[result.index])\n",
        "\n",
        "        for doc_id in doc_ids_reranked:\n",
        "            docs_retrieved.append(\n",
        "                {\n",
        "                    \"title\": self.docs[doc_id][\"title\"],\n",
        "                    \"text\": self.docs[doc_id][\"text\"],\n",
        "                    \"url\": self.docs[doc_id][\"url\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        return docs_retrieved\n",
        "\n",
        "class Chatbot:\n",
        "\n",
        "    def __init__(self, docs: Documents):\n",
        "        self.docs = docs\n",
        "        self.conversation_id = str(uuid.uuid4())\n",
        "\n",
        "    def generate_response(self, message: str):\n",
        "        response = co.chat(message=message, search_queries_only=True)\n",
        "\n",
        "        if response.search_queries:\n",
        "            print(\"Retrieving information...\")\n",
        "            documents = self.retrieve_docs(response)\n",
        "\n",
        "            response = co.chat(\n",
        "                message=message,\n",
        "                documents=documents,\n",
        "                conversation_id=self.conversation_id,\n",
        "                stream=True,\n",
        "            )\n",
        "            for event in response:\n",
        "                yield event\n",
        "            yield response\n",
        "        else:\n",
        "            response = co.chat(\n",
        "                message=message,\n",
        "                conversation_id=self.conversation_id,\n",
        "                stream=True\n",
        "            )\n",
        "            for event in response:\n",
        "                yield event\n",
        "\n",
        "\n",
        "    def retrieve_docs(self, response) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Retrieves documents based on the search queries in the response.\n",
        "\n",
        "        Parameters:\n",
        "        response: The response object containing search queries.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict[str, str]]: A list of dictionaries representing the retrieved documents.\n",
        "\n",
        "        \"\"\"\n",
        "        # Get the query(s)\n",
        "        queries = []\n",
        "        for search_query in response.search_queries:\n",
        "            queries.append(search_query[\"text\"])\n",
        "\n",
        "        # Retrieve documents for each query\n",
        "        retrieved_docs = []\n",
        "        for query in queries:\n",
        "            retrieved_docs.extend(self.docs.retrieve(query))\n",
        "\n",
        "        return retrieved_docs\n",
        "\n",
        "\n",
        "class App:\n",
        "\n",
        "    def __init__(self, chatbot: Chatbot):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the App class.\n",
        "\n",
        "        Parameters:\n",
        "        chatbot (Chatbot): An instance of the Chatbot class.\n",
        "\n",
        "        \"\"\"\n",
        "        self.chatbot = chatbot\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        while True:\n",
        "            # Get the chatbot response\n",
        "            message = input(\"User: \")\n",
        "            response = self.chatbot.generate_response(message)\n",
        "\n",
        "            # Print the chatbot response\n",
        "            print(\"Chatbot:\")\n",
        "            citations_flag = False\n",
        "\n",
        "            for event in response:\n",
        "                stream_type = type(event).__name__\n",
        "\n",
        "                # Text\n",
        "                if stream_type == \"StreamTextGeneration\":\n",
        "                    print(event.text, end=\"\")\n",
        "\n",
        "                # Citations\n",
        "                if stream_type == \"StreamCitationGeneration\":\n",
        "                    if not citations_flag:\n",
        "                        print(\"\\n\\nCITATIONS:\")\n",
        "                        citations_flag = True\n",
        "                    print(event.citations[0])\n",
        "\n",
        "                # Documents\n",
        "                if citations_flag:\n",
        "                    if stream_type == \"StreamingChat\":\n",
        "                        print(\"\\n\\nDOCUMENTS:\")\n",
        "                        documents = [{'id': doc['id'],\n",
        "                                    'text': doc['text'][:50] + '...',\n",
        "                                    'title': doc['title'],\n",
        "                                    'url': doc['url']}\n",
        "                                    for doc in event.documents]\n",
        "                        for doc in documents:\n",
        "                            print(doc)\n",
        "\n",
        "            print(f\"\\n{'-'*100}\\n\")\n"
      ],
      "metadata": {
        "id": "wjD-PY5PrKdN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources = [\n",
        "    {\n",
        "        \"title\": \"Text Embeddings\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/text-embeddings\"},\n",
        "    {\n",
        "        \"title\": \"Similarity Between Words and Sentences\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/similarity-between-words-and-sentences\"},\n",
        "    {\n",
        "        \"title\": \"The Attention Mechanism\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/the-attention-mechanism\"},\n",
        "    {\n",
        "        \"title\": \"Transformer Models\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/transformer-models\"}\n",
        "]\n",
        "\n",
        "documents = Documents(sources)\n",
        "chatbot = Chatbot(documents)\n",
        "app = App(chatbot)\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iMGWsmI98yUJ",
        "outputId": "275d23bc-4812-46fd-d66d-da6f90478cdb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents...\n",
            "Embedding documents...\n",
            "Indexing documents...\n",
            "Indexing complete with 136 documents.\n",
            "User: Hello, I have a question\n",
            "Chatbot:\n",
            "Of course, I'm here to help! What's your question?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "User: What's the difference between word and sentence embeddings\n",
            "Chatbot:\n",
            "Retrieving information...\n",
            "Word embeddings and sentence embeddings are the fundamental components of LLMs and convert language (words) into computer speak (numbers) in a way that preserves the relationships between words, semantics, and linguistic nuances into equations that assign corresponding numbers. \n",
            "\n",
            "A word embedding associates words with lists of numbers (vectors) in a manner that similar words are associated with numbers that are close by and dissimilar words with numbers that are far away from each other. Sentence embeddings do the same thing, but for sentences. This is done by associating a vector to every sentence.\n",
            "\n",
            "CITATIONS:\n",
            "{'start': 0, 'end': 15, 'text': 'Word embeddings', 'document_ids': ['doc_0', 'doc_1', 'doc_2']}\n",
            "{'start': 20, 'end': 39, 'text': 'sentence embeddings', 'document_ids': ['doc_0', 'doc_1', 'doc_2']}\n",
            "{'start': 48, 'end': 78, 'text': 'fundamental components of LLMs', 'document_ids': ['doc_2']}\n",
            "{'start': 83, 'end': 137, 'text': 'convert language (words) into computer speak (numbers)', 'document_ids': ['doc_2']}\n",
            "{'start': 152, 'end': 228, 'text': 'preserves the relationships between words, semantics, and linguistic nuances', 'document_ids': ['doc_2']}\n",
            "{'start': 234, 'end': 278, 'text': 'equations that assign corresponding numbers.', 'document_ids': ['doc_2']}\n",
            "{'start': 283, 'end': 297, 'text': 'word embedding', 'document_ids': ['doc_0']}\n",
            "{'start': 309, 'end': 346, 'text': 'words with lists of numbers (vectors)', 'document_ids': ['doc_0']}\n",
            "{'start': 364, 'end': 420, 'text': 'similar words are associated with numbers that are close', 'document_ids': ['doc_0']}\n",
            "{'start': 428, 'end': 475, 'text': 'dissimilar words with numbers that are far away', 'document_ids': ['doc_0']}\n",
            "{'start': 493, 'end': 512, 'text': 'Sentence embeddings', 'document_ids': ['doc_0', 'doc_1']}\n",
            "{'start': 540, 'end': 550, 'text': 'sentences.', 'document_ids': ['doc_0', 'doc_1']}\n",
            "{'start': 567, 'end': 606, 'text': 'associating a vector to every sentence.', 'document_ids': ['doc_0', 'doc_1']}\n",
            "\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'In the previous chapters, you learned about word a...', 'title': 'The Attention Mechanism', 'url': 'https://docs.cohere.com/docs/the-attention-mechanism'}\n",
            "{'id': 'doc_1', 'text': 'This is where sentence embeddings come into play. ...', 'title': 'Text Embeddings', 'url': 'https://docs.cohere.com/docs/text-embeddings'}\n",
            "{'id': 'doc_2', 'text': 'Conclusion\\n\\nWord and sentence embeddings are the b...', 'title': 'Text Embeddings', 'url': 'https://docs.cohere.com/docs/text-embeddings'}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "User: What do you know about graph neural networks?\n",
            "Chatbot:\n",
            "Retrieving information...\n",
            "Graph neural networks are a subset of neural network-based graphical models. They are a flexible and general framework for modeling pair potentials between nodes in a graph and are designed to understand the relationships between the nodes in the graph and predictions. In fact, Graph Neural Networks have found numerous applications in fields like image recognition, chemistry, robotics, natural language processing, recommender systems, and many more. \n",
            "\n",
            "Would you like me to go into more detail about Graph Neural Networks or explain some of these applications?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ffb27048cd75>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mchatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatbot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-91dc9007e5e0>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# Get the chatbot response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}